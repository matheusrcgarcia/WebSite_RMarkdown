---
title: "Customer Segmentation"
description: |
  In this project, I will be performing an unsupervised clustering of data on the customer's records from a groceries firm's database. Customer segmentation is the practice of separating customers into groups that reflect similarities among customers in each cluster. I will divide customers into segments to optimize the significance of each customer to the business. To modify products according to distinct needs and behaviours of the customers. It also helps the business to cater to the concerns of different types of customers.
---

```{r setup, include=FALSE}
library(wooldridge)
library(tidyverse)
library(texreg)
library(broom)

knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

* W3.C11 Use the data in **MEAPSINGLE** to study the effects of single-parent households on student math performance. These data are for a subset of schools in southeast Michigan for the year 2000. The socioeconomic variables are obtained at the ZIP code level (where ZIP code is assigned to schools based on their mailing addresses).

```{r Data}
df <- meapsingle
```

#
* Part(i)
Run the simple regression of *math4* on *pctsgle* and report the results in the usual format. **Interpret the slope coefficient**. Does the effect of single parenthood seem large or small?

```{r Part (i), results='asis'}
model.1 <- lm(math4~pctsgle,data = df, na.action = na.omit)
htmlreg(model.1,
        stars = c(0.01,0.05,01),
        caption = "math4",
        caption.above = TRUE,
        digits = 3)
```

* The coefficient on *pctsgle* is significant, it has three stars of significance in our linear regression, which mean that the probability of not get this number is less the 1%. 


#
* Part(ii)
Add the variables *lmedinc* and *free* to the equation. What happens to the coefficient on *pctsgle*? Explain what is happening.

```{r Part (ii), results='asis'}
model.2 <- lm(math4~pctsgle+lmedinc+free,data = df, na.action = na.omit)
htmlreg(list(model.1,model.2),
        stars = c(0.01,0.05,01),
        caption = "math4",
        caption.above = TRUE,
        digits = 3)

```

Comparing those both models, we can see by adding the variables *lmedinc* and *free* to the equation that the coefficient on *pctsgle* becomes less significant (decreases almost 75% his significance), and the variable with the highest significance become *free*. 
What is happening is, by omitting the variable *free* and only analyzing the significance of *pctsgle* we have a wrong understanding that this last one is very significant for *math4*. 

#
* Part(iii)
Find the sample correlation between *lmedinc* and *free*. Does it have the sign you expect?
```{r Part iii}
lm_free_cor <- df %>%
  summarise(correlation = cor(lmedinc,free,use="complete.obs")) %>%
  round(digits = 2)
lm_free_cor
```

* The result is not surprising! The highest is the income of the family, less they will need assistance for free launch. 

#
* Part(iv)
Does the substantial correlation between *lmedinc* and *free* mean that you should drop one from the regression to better estimate the causal effect of single parenthood on student performance? Explain.

  * Despite the fact those variables have a substantial correlation, you should not drop any of those in your regression, first because they do not have a perfect (exact) correlation, second because their interaction can (and will) affect our linear regression because they can affect other variables, foe example *pctsgle*.

#
* Part(v)
Find the variance inflation factors (VIFs) for each of the explanatory variables appearing in the regression in part (ii). Which variable has the largest VIF? Does this knowledge affect the model you would use to study the causal effect of single parenthood on math performance?

```{r Part v}
library(car)
vif(model.2)
```
The variable that has the largest VIF is *pctsgle*. 

Knowing this will affect the model that we will use because, we run the model 2, we saw that the variable *pctsgle* is the less significant one on this model, and also has the highest VIF. Therefore we can stop using this one. 



